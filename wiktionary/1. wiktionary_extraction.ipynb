{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372133c8",
   "metadata": {},
   "source": [
    "# JSON Data Extraction\n",
    "In this step, we:\n",
    "1. load JSON file\n",
    "2. save it as a list of dictionaries\n",
    "3. and save it back to new JSON file with an indentation of 4 spaces for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4207cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781134d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file is loaded successfully\n",
      "New JSON file is created successfully\n"
     ]
    }
   ],
   "source": [
    "file_path = './data/kaikki.org-dictionary-English.json'\n",
    "new_file_path = './data/kaikki_formatted.json'\n",
    "\n",
    "# Load the JSON data\n",
    "with open(file_path, encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "    print('JSON file is loaded successfully')\n",
    "\n",
    "# Save the data as JSON to a new file\n",
    "with open(new_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "print('New JSON file is created successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c1dea",
   "metadata": {},
   "source": [
    "# Extracting Translations\n",
    "In this step we:\n",
    "1. extract translations from a JSON file and saves them in a TSV file\n",
    "2. remove duplicate rows from the TSV file based on specific columns and saves the modified data to a new TSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26da37e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations extracted and saved in ./data/senses_extraction.tsv\n",
      "Translations filtered and saved in ./data/senses_extraction_modified.tsv\n"
     ]
    }
   ],
   "source": [
    "file_path = './data/kaikki_formatted.json'\n",
    "output_path = './data/senses_extraction.tsv'\n",
    "\n",
    "languages = {'Indonesian', 'Arabic', 'Mandarin Chinese', 'Greek',\n",
    "             'Portuguese', 'Finnish', 'Spanish', 'Japanese', 'Serbo-Croatian', 'Polish', 'Slovene', 'Thai'}\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Prepare the TSV file for writing\n",
    "with open(output_path, 'w', newline='', encoding='utf-8') as tsv_file:\n",
    "    writer = csv.writer(tsv_file, delimiter='\\t')\n",
    "\n",
    "    # Write the header row\n",
    "    header = ['lang', 'word', 'pos', 'sense']\n",
    "    writer.writerow(header)\n",
    "\n",
    "    rows_to_write = []\n",
    "\n",
    "    # Iterate over each entry in the JSON data\n",
    "    for entry in data:\n",
    "        pos = entry.get('pos')\n",
    "        senses = entry.get('senses', [])\n",
    "\n",
    "        # Extract the English word, sense, and pos if available\n",
    "        english_word = entry.get('word') if entry.get('lang') == 'English' else 'None'\n",
    "\n",
    "        # Iterate over each sense and extract translations\n",
    "        for sense in senses:\n",
    "            if 'translations' in sense:\n",
    "                # Iterate over translations\n",
    "                for translation in sense['translations']:\n",
    "                    lang = translation.get('lang')\n",
    "                    if english_word is not None:\n",
    "                        sense_row = translation.get('sense')\n",
    "                        word_row = translation.get('word')\n",
    "                        rows_to_write.append(['English', english_word, pos, sense_row])\n",
    "                    if lang in languages:\n",
    "                        sense_row = translation.get('sense')\n",
    "                        word_row = translation.get('word')\n",
    "                        rows_to_write.append([lang, word_row, pos, sense_row])\n",
    "\n",
    "        # Check if translations are available outside the senses key\n",
    "        if 'translations' in entry:\n",
    "            for translation in entry['translations']:\n",
    "                lang = translation.get('lang')\n",
    "                word_row = translation.get('word')\n",
    "                if english_word is not None:\n",
    "                    sense_row = translation.get('sense')\n",
    "                    rows_to_write.append(['English', english_word, pos, sense_row])\n",
    "                if lang in languages:\n",
    "                    sense_row = translation.get('sense')\n",
    "                    rows_to_write.append([lang, word_row, pos, sense_row])\n",
    "\n",
    "    # Write all rows at once\n",
    "    writer.writerows(rows_to_write)\n",
    "\n",
    "print('Translations extracted and saved in', output_path)\n",
    "\n",
    "# Start filtering\n",
    "senses_file = './data/senses_extraction.tsv'\n",
    "filtered_file = './data/senses_extraction_modified.tsv'\n",
    "\n",
    "df = pd.read_csv(senses_file, delimiter='\\t').drop_duplicates(subset=['lang', 'pos', 'sense', 'word'])\n",
    "df.to_csv(filtered_file, sep='\\t', index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "print('Translations filtered and saved in', filtered_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb04ba2",
   "metadata": {},
   "source": [
    "# Mapping words based on senses\n",
    "In this step, we:\n",
    "1. filter and process the data based on certain conditions\n",
    "3. map translations (words) to each sense by organizing them in a dictionary\n",
    "2. generate a new TSV file with labeled rows containing words for specific senses and languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0638ddbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation mapped and saved in ./data/output_senses.tsv\n"
     ]
    }
   ],
   "source": [
    "filename = './data/senses_extraction_modified.tsv'\n",
    "output_path = './data/output_senses.tsv'\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as tsv_file:\n",
    "    reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "    labeled_rows = {}\n",
    "    completed_senses = set()\n",
    "    # Iterate over the rows\n",
    "    for row in reader:\n",
    "        lang = row[0]\n",
    "        word = row[1]\n",
    "        sense = row[3]\n",
    "        pos = row[2]  # New line to extract the POS\n",
    "        # Check if the sense has already been completed\n",
    "        if sense in completed_senses:\n",
    "            continue\n",
    "        # Check if the sense already exists in the labeled rows\n",
    "        if sense in labeled_rows:\n",
    "            data = labeled_rows[sense]\n",
    "            # Check if the word exists in any language for the sense\n",
    "            if any(word in values for values in data.values()):\n",
    "                continue\n",
    "        else:\n",
    "            data = {}\n",
    "\n",
    "        # Add the word and POS to the labeled rows\n",
    "        data.setdefault(lang, []).append(word)\n",
    "        data['POS'] = pos  # New line to add the POS\n",
    "        labeled_rows[sense] = data\n",
    "\n",
    "        # Check if the sense now has words for all 13 languages\n",
    "        if len(data) == 14:  # Adjusted the condition to account for the added POS\n",
    "            completed_senses.add(sense)\n",
    "            if len(completed_senses) == len(labeled_rows):\n",
    "                break\n",
    "    # Remove rows where there is no content in the Indonesian row\n",
    "    labeled_rows = {sense: data for sense, data in labeled_rows.items() if 'Indonesian' in data}\n",
    "\n",
    "    # Create a new TSV file to save the labeled rows\n",
    "    with open(output_path, 'w', encoding='utf-8', newline='') as output_file:\n",
    "        writer = csv.writer(output_file, delimiter='\\t')\n",
    "        languages = sorted(set(lang for data in labeled_rows.values() for lang in data.keys()))\n",
    "        writer.writerow(['Sense'] + languages + ['POS'])  # Added 'POS' to the header row\n",
    "        # Write the labeled rows\n",
    "        for sense, data in labeled_rows.items():\n",
    "            row = [sense]\n",
    "            for lang in languages:\n",
    "                words = ' '.join(data.get(lang, []))\n",
    "                if not words:\n",
    "                    words = 'None'\n",
    "                row.append(words)\n",
    "            row.append(data.get('POS', 'None'))  # Added the POS to the row\n",
    "            writer.writerow(row)\n",
    "\n",
    "print('Translation mapped and saved in', output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92299185",
   "metadata": {},
   "source": [
    "# Start filtering process \n",
    "In this step, we:\n",
    "1. process its content\n",
    "2. generate a new TSV file with the same header row and filtered data where each field contains either the first word from the original field, the single word itself, or 'None' if there are no words\n",
    "3. the last row 'lang' is then manually deleted and the 'Indonesian' row is moved to the first row for intersection alogrithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cb9d51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation mapped and saved in ./data/output.tsv\n"
     ]
    }
   ],
   "source": [
    "# Increase the field size limit\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "def process_tsv_file(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Function to process a TSV file, extracting the first word from each cell of the second and subsequent columns.\n",
    "\n",
    "    Param:\n",
    "        input_file (str): The path to the input TSV file to be processed.\n",
    "        output_file (str): The path to the output TSV file to save the processed data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8', newline='') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        header = next(reader)  \n",
    "        output_data = [header[1:]]  \n",
    "\n",
    "        # Process each row except the first one\n",
    "        for row in reader:\n",
    "            new_row = []\n",
    "            for item in row[1:]:  \n",
    "                words = item.split()  \n",
    "                if len(words) > 1:\n",
    "                    new_row.append(words[0])  \n",
    "                elif len(words) == 1:\n",
    "                    new_row.append(words[0])  \n",
    "                else:\n",
    "                    new_row.append(None)  \n",
    "            output_data.append(new_row)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter='\\t')\n",
    "        writer.writerows(output_data)  \n",
    "\n",
    "input_file = './data/output_senses.tsv'\n",
    "output_file = './data/output.tsv'\n",
    "process_tsv_file(input_file, output_file)\n",
    "\n",
    "print('Translation mapped and saved in', output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda4e97",
   "metadata": {},
   "source": [
    "# Dropping rows and move Indonesian row to the first one\n",
    "In this step we:\n",
    "1. drop rows that we are not going to use in the intersection\n",
    "2. move Indonesian row to the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17262e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows dropped and saved in ./data/output_wiktionary.tsv\n"
     ]
    }
   ],
   "source": [
    "tsv_file_path = './data/output.tsv'\n",
    "\n",
    "df = pd.read_csv(tsv_file_path, sep='\\t')\n",
    "df = df.drop(['POS', 'lang', 'POS'], axis=1)\n",
    "df = df.drop(['POS.1'], axis=1)\n",
    "\n",
    "output_file_path = './data/output_wiktionary.tsv'\n",
    "\n",
    "# Find the index of the Indonesian row\n",
    "indonesian_index = df.columns.get_loc('Indonesian')\n",
    "\n",
    "# Move the Indonesian row to the first position\n",
    "df = df[[df.columns[indonesian_index]] + list(df.columns[:indonesian_index]) + list(df.columns[indonesian_index+1:])]\n",
    "\n",
    "# Save the modified DataFrame back to a file\n",
    "df.to_csv('./data/output_wiktionary.tsv', index=False, sep='\\t')\n",
    "\n",
    "print('Rows dropped and saved in', output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
