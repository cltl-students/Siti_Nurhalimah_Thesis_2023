{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a56b4ca",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8070e72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package extended_omw to\n",
      "[nltk_data]     /Users/sitinurhalimah/nltk_data...\n",
      "[nltk_data]   Package extended_omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import csv\n",
    "import json\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('extended_omw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1233703",
   "metadata": {},
   "source": [
    "# Loop through to find synset for each language\n",
    "In this step, we:\n",
    "1. retrieve synset suggestions for each word in the supported WordNet languages\n",
    "2. save the suggestions in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "880e31a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offset suggestions saved to './data/output_wiktionary.json' successfully.\n"
     ]
    }
   ],
   "source": [
    "# Increase the field size limit\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "language_codes_mapping = {\n",
    "    'Indonesian': 'ind',\n",
    "    'Arabic': 'arb',\n",
    "    'Mandarin Chinese': 'cmn',\n",
    "    'Greek': 'ell',\n",
    "    'English': 'eng',\n",
    "    'Portuguese': 'por',\n",
    "    'Finnish': 'fin',\n",
    "    'Spanish': 'spa',\n",
    "    'Japanese': 'jpn',\n",
    "    'Serbo-Croatian': 'hrv',\n",
    "    'Polish': 'pol',\n",
    "    'Slovene': 'slv',\n",
    "    'Thai': 'tha'\n",
    "}\n",
    "\n",
    "def get_synset_suggestions(word, language_code):\n",
    "    \"\"\"\n",
    "    Function to get synset suggestions for a given word and language.\n",
    "    Param:\n",
    "        word (str): The word for which to find synsets.\n",
    "        language_code (str): The language code for the synsets (e.g., 'eng' for English, 'spa' for Spanish).\n",
    "    Returns:\n",
    "        list: A list of synset suggestions, where each suggestion is in the format \"{language_code}: {offset}\".\n",
    "              If no synsets are found or an error occurs, an empty list is returned.\n",
    "    \"\"\"\n",
    "    synset_suggestions = []\n",
    "    try:\n",
    "        synsets = wn.synsets(word, lang=language_code)\n",
    "        for synset in synsets:\n",
    "            offset = str(synset.offset()).zfill(8) + '-' + synset.pos()\n",
    "            synset_suggestions.append(f\"{language_code}: {offset}\")\n",
    "    except (Exception, IndexError):\n",
    "        pass  # Ignore errors when synsets are not found\n",
    "    return synset_suggestions\n",
    "\n",
    "data_path = './data/output_wiktionary.tsv'\n",
    "output_path = './data/output_wiktionary.json'\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    headers = next(reader)\n",
    "    language_codes = headers[1:]  # Exclude the 'id' column\n",
    "    data = list(reader)\n",
    "\n",
    "output_data = {}\n",
    "\n",
    "for row in data:\n",
    "    word = row[0]\n",
    "    synset_suggestions = {}\n",
    "    for i, language_code in enumerate(language_codes):\n",
    "        if language_code in language_codes_mapping:\n",
    "            wordnet_language = language_codes_mapping[language_code]\n",
    "            synsets = get_synset_suggestions(row[i+1], wordnet_language)\n",
    "            if synsets:\n",
    "                synset_suggestions[language_code] = synsets\n",
    "\n",
    "    if synset_suggestions:\n",
    "        output_data[word] = synset_suggestions\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(output_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Offset suggestions saved to '{output_path}' successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5254370",
   "metadata": {},
   "source": [
    "# Merged results to see suggested synsets\n",
    "In this step, we:\n",
    "1. process the synset suggestions obtained from the JSON file\n",
    "2. consolidate them, and generate a count of how many languages suggest each synset for each Indonesian word\n",
    "3. the resulting count data is then saved in a TSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bdff1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset suggestions count saved to ./data/synsets_output_wiktionary.tsv\n"
     ]
    }
   ],
   "source": [
    "with open('./data/output_wiktionary.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "count_data = []\n",
    "for indonesian_word, suggestions in data.items():\n",
    "    merged_synsets = {}\n",
    "    for language, synsets in suggestions.items():\n",
    "        for synset in synsets:\n",
    "            synset_key = synset.split(':')[1].strip()\n",
    "            if synset_key not in merged_synsets:\n",
    "                merged_synsets[synset_key] = set()\n",
    "\n",
    "            merged_synsets[synset_key].add(language)\n",
    "\n",
    "    for synset, languages in merged_synsets.items():\n",
    "        count = len(languages)\n",
    "        count_data.append({\n",
    "            'synset': synset,\n",
    "            'language': ', '.join(languages),\n",
    "            'lemma': indonesian_word,\n",
    "            'count': count\n",
    "        })\n",
    "\n",
    "filename = './data/synsets_output_wiktionary.tsv'\n",
    "\n",
    "with open(filename, 'w', newline='', encoding='utf-8') as tsv_file:\n",
    "    writer = csv.writer(tsv_file, delimiter='\\t')\n",
    "    writer.writerow(['synset', 'language', 'lemma', 'count'])\n",
    "\n",
    "    for item in count_data:\n",
    "        writer.writerow([item['synset'], item['language'], item['lemma'], item['count']])\n",
    "\n",
    "print('Synset suggestions count saved to', filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db56ce2",
   "metadata": {},
   "source": [
    "# Adding goodness labels\n",
    "In this step, we:\n",
    "1. add goodness labels (Y, O, X and L) into the suggested synsets if they are match with the ones found in master data obtained from SourceForge (https://sourceforge.net/p/wn-msa/tab/HEAD/tree/trunk/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86561097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated TSV file saved as ./data/synset_output_wiktionary_with_labels.tsv\n"
     ]
    }
   ],
   "source": [
    "synset_tsv_file= './data/synsets_output_wiktionary.tsv'\n",
    "main_data_tsv_file = './data/wn_msa_data.tsv'\n",
    "output_tsv_file = './data/synset_output_wiktionary_with_labels.tsv'\n",
    "\n",
    "\n",
    "labels = {}\n",
    "with open(main_data_tsv_file, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    next(reader)  \n",
    "    for row in reader:\n",
    "        synset = row[0]\n",
    "        label = row[1]\n",
    "        lemma = row[2]\n",
    "        labels[(synset, lemma)] = label\n",
    "\n",
    "# Update the first TSV file with the labels\n",
    "with open(synset_tsv_file, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    header = next(reader)  \n",
    "    header.append('goodness labels')  \n",
    "    rows = []\n",
    "    for row in reader:\n",
    "        synset = row[0]\n",
    "        lemma = row[2]\n",
    "        if (synset, lemma) in labels:\n",
    "            row.append(labels[(synset, lemma)])\n",
    "        else:\n",
    "            row.append('None')\n",
    "        rows.append(row)\n",
    "\n",
    "# Save the updated data to the output TSV file\n",
    "with open(output_tsv_file, 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    writer.writerow(header)  \n",
    "    writer.writerows(rows) \n",
    "\n",
    "print('Updated TSV file saved as', output_tsv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a13f82",
   "metadata": {},
   "source": [
    "# Merging File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f12adabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSV files merged and saved as: ./data/merged_file_wiktionary.tsv\n"
     ]
    }
   ],
   "source": [
    "def merge_tsv(tsv_file1, tsv_file2, output_file):\n",
    "    \"\"\"\n",
    "    Function to merge data from two TSV files based on a common 'synset' and 'lemma' fields,\n",
    "    and create a new TSV file containing the merged data.\n",
    "\n",
    "    Param:\n",
    "        tsv_file1 (str): The path to the first input TSV file.\n",
    "        tsv_file2 (str): The path to the second input TSV file.\n",
    "        output_file (str): The path to the output TSV file to save the merged data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    merged_rows = []\n",
    "\n",
    "    with open(tsv_file1, 'r', encoding='utf-8') as file1, open(tsv_file2, 'r', encoding='utf-8') as file2:\n",
    "        reader1 = csv.DictReader(file1, delimiter='\\t')\n",
    "        reader2 = csv.DictReader(file2, delimiter='\\t')\n",
    "        \n",
    "        # Create a dictionary to store rows from tsv_file1 for quick lookup\n",
    "        tsv1_rows = {row['synset']: row for row in reader1}\n",
    "\n",
    "        for row2 in reader2:\n",
    "            synset = row2['synset']\n",
    "            if synset in tsv1_rows:\n",
    "                row1 = tsv1_rows[synset]\n",
    "                confidence = int(row1['count'])\n",
    "                language = row1['language']\n",
    "            else:\n",
    "                confidence = 'None'\n",
    "                language = 'None'\n",
    "\n",
    "            merged_row = {\n",
    "                'synset': synset,\n",
    "                'lemma': row2['lemma'],\n",
    "                'annotation': row2['annotation'],\n",
    "                'goodness label': row2['goodness label'],\n",
    "                'confidence': confidence,\n",
    "                'language': language\n",
    "            }\n",
    "            merged_rows.append(merged_row)\n",
    "\n",
    "    fieldnames = ['synset', 'lemma', 'annotation', 'goodness label', 'confidence', 'language']\n",
    "    with open(output_file, 'w', encoding='utf-8', newline='') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "        writer.writeheader()\n",
    "        writer.writerows(merged_rows)\n",
    "\n",
    "tsv_file1 = './data/synset_output_wiktionary_with_labels.tsv'\n",
    "tsv_file2 = './data/development_set_with_labels.tsv'\n",
    "output_file = './data/merged_file_wiktionary.tsv'\n",
    "\n",
    "merge_tsv(tsv_file1, tsv_file2, output_file)\n",
    "print(\"TSV files merged and saved as:\", output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
