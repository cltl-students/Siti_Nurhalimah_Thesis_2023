{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0fe5a3a",
   "metadata": {},
   "source": [
    "The code to extract parallel corpus from Wiktionary has been solved and output file has been saved correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8308860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "file_path = './data/kaikki.org-dictionary-English.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b924b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the 13 languages (including Indonesian) to extract translations for\n",
    "# Chinese contains russian translations, switch to Mandarin Chinese\n",
    "# {\"lang\": \"Chinese\", \"code\": \"dng\", \"tags\": [\"Dungan\"], \n",
    "#\"sense\": \"time period of sixty minutes\", \"roman\": \"sahatɨ\", \"word\": \"сахаты\", \"_dis1\": \"59 10 19 8 2 1\"}\n",
    "# Swicth Croatian to Serbo-Croatian, translation empty\n",
    "# languages = ['Indonesian', 'Arabic', 'Mandarin Chinese', 'Greek', 'English', 'Persian', 'Finnish', 'Spanish', 'Japanese', 'Serbo-Croatian', 'Polish', 'Slovene', 'Thai']\n",
    "\n",
    "languages = ['id', 'ar', 'cmn', 'el', 'en', 'fa', 'fi', 'es', 'ja', 'sh', 'pl', 'sl', 'th']\n",
    "\n",
    "# Create a dictionary to store the translations for each language\n",
    "translations = {}\n",
    "for language in languages:\n",
    "    translations[language] = {}\n",
    "\n",
    "# Load the JSON data\n",
    "with open(file_path, encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "    print('JSON file is loaded successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the translations for each language\n",
    "for entry in data:\n",
    "    senses = entry.get('senses', [])\n",
    "    for sense in senses:\n",
    "        for language in languages:\n",
    "            translations_list = sense.get('translations', [])\n",
    "            for translation in translations_list:\n",
    "                if translation.get('lang') == language:\n",
    "                    concept = entry['word']\n",
    "                    form = translation.get('word')\n",
    "                    if form is not None:\n",
    "                        if concept not in translations[language]:\n",
    "                            translations[language][concept] = [form]\n",
    "                        else:\n",
    "                            translations[language][concept].append(form)\n",
    "\n",
    "# print the translations for each language\n",
    "for language in languages:\n",
    "    print(f'Translations for {language}')\n",
    "    for concept in translations[language]:\n",
    "        forms = ', '.join(translations[language][concept])\n",
    "        # print(f'{concept}: {forms}')\n",
    "    print(f'Finished collecting translations for {language}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba4934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_path = './data/translations_output_wiktionary.tsv'\n",
    "\n",
    "# Write the translations to a TSV file\n",
    "with open(output_path, mode='w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    writer.writerow(['Indonesian'] + languages[1:])  # Use Indonesian as the first row\n",
    "\n",
    "    for concept in translations[languages[0]]:\n",
    "        row_data = []\n",
    "        for language in languages:\n",
    "            translations_list = translations[language].get(concept, ['None'])\n",
    "            if language == 'English':\n",
    "                row_data.append(concept)\n",
    "            else:\n",
    "                row_data.append(', '.join(translations_list))\n",
    "        writer.writerow(row_data)\n",
    "        print(row_data)\n",
    "\n",
    "print(f'Translations saved to {output_path}')\n",
    "\n",
    "# Read the TSV file as a dataframe\n",
    "df = pd.read_csv(output_path, delimiter='\\t')\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e561b405",
   "metadata": {},
   "source": [
    "Data cleaning:\n",
    "1. Checking the language ID\n",
    "2. Mapping the Concept as English translations \n",
    "3. Checking if all words are translated correctly, for example, in the JSON kaikki file, translation for language 'Chinese' has some russian words such as \"太陽, 太阳, 日, эрту, жәту, тэён, 日頭, 日头, 太陽, 太阳, 日頭, 日头, 太陽, 太阳, 太陽, 太阳, 日頭, 日头, 太陽, 太阳, 日頭, 日头, 熱頭, 热头, эрту, жәту, 日頭, 日头, 太陽, 太阳, 日頭, 日头, 太陽, 太阳, 太陽, 太阳, 日頭, 日头, 曝\" for the translation \"matahari, matahari, surya\" so instead of using Chinese we should use Mandarin Chinese. Alos, Croation isn't available in the JSON file and have to be switched to Serbo-Croatian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6988470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final code with lang_code\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "languages = ['Indonesian', 'Arabic', 'Mandarin Chinese', 'Greek', 'English',\n",
    "             'Persian', 'Finnish', 'Spanish', 'Japanese', 'Serbo-Croatian', 'Polish', 'Slovene', 'Thai']\n",
    "\n",
    "language_codes = ['id', 'ar', 'cmn', 'el', 'en', 'fa', 'fi', 'es', 'ja', 'sh', 'pl', 'sl', 'th']\n",
    "\n",
    "# Create a dictionary to store the translations and POS for each language\n",
    "translations = {}\n",
    "for language in languages:\n",
    "    translations[language] = {}\n",
    "\n",
    "\n",
    "# Extract the translations and POS for each language\n",
    "for entry in data:\n",
    "    for language, language_code in zip(languages, language_codes):\n",
    "        senses = entry.get('senses', [])\n",
    "        for sense in senses:\n",
    "            translations_list = sense.get('translations', [])\n",
    "            pos = entry.get('pos')\n",
    "            for translation in translations_list:\n",
    "                if translation.get('lang') == language:\n",
    "                    concept = entry['word']\n",
    "                    form = translation.get('word')\n",
    "                    if form is not None:\n",
    "                        if concept not in translations[language]:\n",
    "                            translations[language][concept] = {'word': form, 'pos': pos}\n",
    "                        else:\n",
    "                            # Check if the POS matches, otherwise set it to 'None'\n",
    "                            if translations[language][concept]['pos'] != pos:\n",
    "                                translations[language][concept]['pos'] = 'None'\n",
    "                            # Update the translation form only if it is not already present\n",
    "                            if form not in translations[language][concept]['word']:\n",
    "                                translations[language][concept]['word'] += ', ' + form\n",
    "\n",
    "output_path = '/content/drive/MyDrive/A Thesis 2023/coding/wiktionary/data/translations_output_wiktionary_backup.tsv'\n",
    "\n",
    "# Write the translations and POS to a TSV file\n",
    "with open(output_path, mode='w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    writer.writerow([language_codes[0]] + language_codes[1:])  # Use language codes as the first row\n",
    "\n",
    "    for concept in translations[languages[0]]:\n",
    "        row_data = []\n",
    "        for language, language_code in zip(languages, language_codes):\n",
    "            translation = translations[language].get(concept, {'word': 'None', 'pos': 'None'})\n",
    "            if language == 'English':\n",
    "                row_data.append(concept)\n",
    "            else:\n",
    "                row_data.append(', '.join(translations_list))\n",
    "            row_data.append(translation['word'].split(',')[0] + '(' + translation['pos'] + ')')\n",
    "        writer.writerow(row_data)\n",
    "        print(row_data)\n",
    "\n",
    "print(f'Translations saved to {output_path}')\n",
    "\n",
    "# Read the TSV file as a dataframe\n",
    "df = pd.read_csv(output_path, delimiter='\\t')\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
