{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da091e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b593a41d",
   "metadata": {},
   "source": [
    "## Removing duplicates from second and third file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b826fa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries removed and saved as ./data/omw-wnbahasa-deleted-cleaned.tsv\n",
      "===================\n",
      "Duplicate entries removed and saved as ./data/ntumc-omw-wnbahasa-used-cleaned.tsv\n"
     ]
    }
   ],
   "source": [
    "#Second file\n",
    "\n",
    "with open('./data/omw-wnbahasa-deleted-indonesian.tsv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    data = list(reader)  \n",
    "\n",
    "unique_entries = []\n",
    "seen_entries = set()\n",
    "for row in data[1:]:  \n",
    "    sense_lemma = (row[0], row[1])\n",
    "    if sense_lemma not in seen_entries:\n",
    "        unique_entries.append(row)\n",
    "        seen_entries.add(sense_lemma)\n",
    "\n",
    "with open('./data/omw-wnbahasa-deleted-cleaned.tsv', 'w', encoding='utf-8', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter='\\t')\n",
    "    writer.writerow(['synset_old', 'lemma', 'src_old', 'usr_old'])  # Write the header row\n",
    "    writer.writerows(unique_entries)\n",
    "\n",
    "print('Duplicate entries removed and saved as ./data/omw-wnbahasa-deleted-cleaned.tsv')\n",
    "print('===================')\n",
    "\n",
    "# Third file\n",
    "with open( './data/ntumc-omw-wnbahasa-used.tsv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    data = list(reader)  \n",
    "\n",
    "unique_entries = []\n",
    "seen_entries = set()\n",
    "for row in data[1:]: \n",
    "    sense_lemma = (row[0], row[1])\n",
    "    if sense_lemma not in seen_entries:\n",
    "        unique_entries.append(row)\n",
    "        seen_entries.add(sense_lemma)\n",
    "\n",
    "with open('./data/ntumc-omw-wnbahasa-used-cleaned.tsv', 'w', encoding='utf-8', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter='\\t')\n",
    "    writer.writerow(['tag', 'clemma'])  \n",
    "    writer.writerows(unique_entries)\n",
    "\n",
    "print('Duplicate entries removed and saved as ./data/ntumc-omw-wnbahasa-used-cleaned.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054885e1",
   "metadata": {},
   "source": [
    "## Combining first, second and third files for development and evaluation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4d8cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development set saved to: ./data/development_data.tsv\n",
      "Evaluation set saved to: ./data/evaluation_data.tsv\n"
     ]
    }
   ],
   "source": [
    "first_file = './data/omw-wn-bahasa-indonesian.tsv'\n",
    "second_file = './data/omw-wnbahasa-deleted-cleaned.tsv'\n",
    "third_file = './data/ntumc-omw-wnbahasa-used-cleaned.tsv'\n",
    "\n",
    "used_data = []\n",
    "deleted_data = []\n",
    "indonesian_data = []\n",
    "\n",
    "with open(third_file, 'r') as file:\n",
    "    next(file) \n",
    "    for line in file:\n",
    "        tag, clemma = line.strip().split('\\t')\n",
    "        used_data.append((tag, clemma, 'KEEP'))\n",
    "\n",
    "with open(second_file, 'r') as file:\n",
    "    next(file) \n",
    "    for line in file:\n",
    "        synset_old, lemma, src_old, usr_old = line.strip().split('\\t')\n",
    "        deleted_data.append((synset_old, lemma, 'DELETE'))\n",
    "\n",
    "with open(first_file, 'r') as file:\n",
    "    next(file)  \n",
    "    for line in file:\n",
    "        row = line.strip().split('\\t')\n",
    "        if len(row) == 4:\n",
    "            synset, lemma, src, confidence = row\n",
    "            usr = ''\n",
    "        else:\n",
    "            synset, lemma, src, confidence, usr = row\n",
    "\n",
    "        if src == 'ntumc':\n",
    "            indonesian_data.append((synset, lemma, 'KEEP'))\n",
    "\n",
    "# Shuffle the KEEP and DELETE rows separately\n",
    "random.shuffle(used_data)\n",
    "random.shuffle(deleted_data)\n",
    "random.shuffle(indonesian_data)\n",
    "\n",
    "combined_data = used_data + deleted_data + indonesian_data\n",
    "\n",
    "# Shuffle the combined list\n",
    "random.shuffle(combined_data)\n",
    "\n",
    "split_index = int(len(combined_data) * 0.6)\n",
    "\n",
    "development_set = combined_data[:split_index]\n",
    "evaluation_set = combined_data[split_index:]\n",
    "\n",
    "# Add headers to the sets\n",
    "development_set.insert(0, ('synset', 'lemma', 'annotation'))\n",
    "evaluation_set.insert(0, ('synset', 'lemma', 'annotation'))\n",
    "\n",
    "development_file_path = './data/development_data.tsv'\n",
    "with open(development_file_path, 'w') as file:\n",
    "    writer = csv.writer(file, delimiter='\\t')\n",
    "    writer.writerows(development_set)\n",
    "print(f'Development set saved to: {development_file_path}')\n",
    "\n",
    "evaluation_file_path = './data/evaluation_data.tsv'\n",
    "with open(evaluation_file_path, 'w') as file:\n",
    "    writer = csv.writer(file, delimiter='\\t')\n",
    "    writer.writerows(evaluation_set)\n",
    "    \n",
    "print(f'Evaluation set saved to: {evaluation_file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9fb162",
   "metadata": {},
   "source": [
    "# Remove duplicates in development data and evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4373f4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to: ./data/development_set.tsv\n",
      "Data saved to: ./data/evaluation_set.tsv\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicates(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Function to remove duplicate rows from the input TSV file for labels with 'KEEP' annotation.\n",
    "\n",
    "    Parame:\n",
    "        input_file (str): The path to the input TSV file containing data in the format of (synset, lemma, annotation).\n",
    "        output_file (str): The path to the output TSV file where the updated data will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    seen_keep = set()\n",
    "    with open(input_file, 'r', newline='') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        header = next(reader)  \n",
    "        for row in reader:\n",
    "            synset, lemma, annotation = row\n",
    "            if annotation == 'KEEP' and (synset, lemma) not in seen_keep:\n",
    "                data.append(row)\n",
    "                seen_keep.add((synset, lemma))\n",
    "            elif annotation == 'DELETE':\n",
    "                data.append(row)\n",
    "\n",
    "    with open(output_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter='\\t')\n",
    "        writer.writerows([header])  \n",
    "        writer.writerows(data) \n",
    "\n",
    "    print(f'Data saved to: {output_file}')\n",
    "\n",
    "development_input_file = './data/development_data.tsv'\n",
    "development_output_file = './data/development_set.tsv'\n",
    "\n",
    "evaluation_input_file = './data/evaluation_data.tsv'\n",
    "evaluation_output_file = './data/evaluation_set.tsv'\n",
    "\n",
    "# Remove duplicates for development data\n",
    "remove_duplicates(development_input_file, development_output_file)\n",
    "# Remove duplicates for evaluation data\n",
    "remove_duplicates(evaluation_input_file, evaluation_output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae93b57",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "In this step, we are analyzing the goodness labels and other data analysis in the purpose of formulating better conditions for the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb396f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Set\n",
      "Total rows: 4924\n",
      "KEEP count: 4097\n",
      "DELETE count: 827\n",
      "Distinct synsets: 3573\n",
      "Distinct lemmas: 3446\n",
      "======================================================================\n",
      "Development Set\n",
      "Total rows: 7346\n",
      "KEEP count: 6131\n",
      "DELETE count: 1215\n",
      "Distinct synsets: 4898\n",
      "Distinct lemmas: 4772\n"
     ]
    }
   ],
   "source": [
    "def analysis(file_path):\n",
    "    \"\"\"\n",
    "    Function to perform analysis on a dataset file.\n",
    "\n",
    "    This function reads a dataset file located at the given file_path, and performs analysis on its content. It calculates\n",
    "    the total number of rows in the file, the count of rows with 'KEEP' annotation, and the count of rows with 'DELETE'\n",
    "    annotation. It also counts the distinct synsets and lemmas found in the dataset.\n",
    "\n",
    "    Param:\n",
    "        file_path (str): The path to the dataset file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    total_rows = 0\n",
    "    keep_count = 0\n",
    "    delete_count = 0\n",
    "    synset_set = set()\n",
    "    lemma_set = set()\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        next(file)\n",
    "        for line in file:\n",
    "            synset, lemma, annotation = line.strip().split('\\t')\n",
    "            if annotation == 'KEEP':\n",
    "                keep_count += 1\n",
    "            elif annotation == 'DELETE':\n",
    "                delete_count += 1\n",
    "            synset_set.add(synset)\n",
    "            lemma_set.add(lemma)\n",
    "\n",
    "            total_rows += 1\n",
    "    print(f'Total rows: {total_rows}')\n",
    "    print(f'KEEP count: {keep_count}')\n",
    "    print(f'DELETE count: {delete_count}')\n",
    "    print(f'Distinct synsets: {len(synset_set)}')\n",
    "    print(f'Distinct lemmas: {len(lemma_set)}')\n",
    "\n",
    "print('Evaluation Set')\n",
    "file_path = './data/evaluation_set.tsv'\n",
    "analysis(file_path)\n",
    "\n",
    "print('======================================================================')\n",
    "print('Development Set')\n",
    "file_path = './data/development_set.tsv'\n",
    "analysis(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
